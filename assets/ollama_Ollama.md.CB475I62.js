import{_ as a,c as l,o as h,ag as i,j as e}from"./chunks/framework.oP1PDRBo.js";const g=JSON.parse('{"title":"Ollama","description":"","frontmatter":{"title":"Ollama","date":"2025-02-10T00:00:00.000Z","tags":["Ollama","大语言模型"],"categories":["大语言模型"],"sticky":3},"headers":[],"relativePath":"ollama/Ollama.md","filePath":"ollama/Ollama.md","lastUpdated":1744012294000}'),t={name:"ollama/Ollama.md"};function p(n,s,k,r,d,E){return h(),l("div",null,s[0]||(s[0]=[i('<p>✨难度：★★★☆☆</p><h2 id="启动并运行大语言模型" tabindex="-1">启动并运行大语言模型 <a class="header-anchor" href="#启动并运行大语言模型" aria-label="Permalink to &quot;启动并运行大语言模型&quot;">​</a></h2><p>启动并运行大语言模型 本地运行DeepSeek-R1，Llama 3.3，Phi-4，Mistral，Gemma 2等大模型</p><p>⭐国内镜像下载地址<a href="https://ollama.zhike.in/" target="_blank" rel="noreferrer">https://ollama.zhike.in/</a></p><p>Ollama官网提供的是Github下载地址，由于Github无法打开或打开很慢，导致下载速度很慢，通过对官网的下载地址进行加速，提供更为快速稳定的下载体验</p><h2 id="部署deepseek-r1模型" tabindex="-1">部署DeepSeek R1模型 <a class="header-anchor" href="#部署deepseek-r1模型" aria-label="Permalink to &quot;部署DeepSeek R1模型&quot;">​</a></h2><p>在终端输入命令：ollama run deepseek-r1 默认是7B版本（4.7GB），如果你的设备配置高，可以尝试70B版本（需要24GB+显存）。</p><p>下载完成后，直接输入：ollama run deepseek-r1 运行DeepSeek R1，然后就可以开始和AI对话啦！</p><h3 id="常用命令" tabindex="-1">常用命令 <a class="header-anchor" href="#常用命令" aria-label="Permalink to &quot;常用命令&quot;">​</a></h3><ul><li>启动Ollama服务： ollama serve</li><li>从模型文件创建模型： ollama create</li><li>显示模型信息： ollama show</li><li>运行模型： ollama run 模型名称</li><li>从注册表中拉去模型： ollama pull 模型名称</li><li>将模型推送到注册表： ollama push</li><li>列出模型： ollama list</li><li>复制模型： ollama cp</li><li>删除模型： ollama rm 模型名称</li><li>获取有关Ollama任何命令的帮助信息： ollama help</li></ul><h3 id="终端指令" tabindex="-1">终端指令 <a class="header-anchor" href="#终端指令" aria-label="Permalink to &quot;终端指令&quot;">​</a></h3><ol><li>查看支持的指令：使用命令 /?</li><li>退出对话模型：使用命令 /bye</li><li>显示模型信息：使用命令 /show</li><li>设置对话参数：使用命令 /set 参数名 参数值，例如设置温度（temperature）或top_k值</li><li>清理上下文：使用命令 /clear</li><li>动态切换模型：使用命令 /load 模型名称</li><li>存储模型：使用命令 /save 模型名称</li><li>查看快捷键：使用命令 /?shortcuts</li></ol><h3 id="可视化界面" tabindex="-1">可视化界面 <a class="header-anchor" href="#可视化界面" aria-label="Permalink to &quot;可视化界面&quot;">​</a></h3><ul><li>如果你觉得命令行不够友好，可以搭配Open Web UI、AnythingLLM等使用！</li><li>以Open Web UI为例，安装Docker后，运行命令：</li></ul><p>在终端运行命令：</p><div class="language-powershell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">powershell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">docker run </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">d </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">p </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">8080</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> --</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">name </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">open-webui</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> --</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">restart always ghcr.io</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">open-webui</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">open-webui</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:main</span></span></code></pre></div><ul><li>打开浏览器，访问，选择DeepSeek R1模型，就能在可视化界面中愉快地聊天啦！</li></ul><div class="language-powershell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">powershell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">访问http:</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">//</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">localhost:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3000</span></span></code></pre></div><p>Ollama是建立在llama.cpp开源推理引擎基础上的大模型推理工具框架。得益于底层引擎提供的高效模型推理，以及多硬件适配，Ollama能够在包括CPU、GPU在内的，不同的硬件环境上，运行各种精度的GGUF格式大模型。通过一个命令行就能拉起LLM模型服务。</p><p>ModelScope社区上托管了数千个优质的GGUF格式的大模型（包括LLM和视觉多模态模型），并支持了Ollama框架和ModelScope平台的链接，通过简单的 <code>ollama run</code>命令，就能<strong>直接加载运行ModelScope模型库上的GGUF模型</strong>。</p><h2 id="一键运行" tabindex="-1">一键运行 <a class="header-anchor" href="#一键运行" aria-label="Permalink to &quot;一键运行&quot;">​</a></h2><div class="language-powershell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">powershell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ollama run modelscope.cn</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Qwen</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Qwen2.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">3B</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Instruct</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">GGUF</span></span></code></pre></div><p>在安装了Ollama的环境(建议使用&gt;=0.3.12版本)上，直接通过上面的命令行，就可以直接在本地运行 Qwen2.5-3B-Instruct-GGUF模型。</p>',23),e("p",{model:""},"ollama run modelscope.cn/{username}/",-1),i(`<div class="language-powershell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">powershell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ollama run modelscope.cn</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Qwen</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Qwen2.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">3B</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Instruct</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">GGUF</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ollama run modelscope.cn</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">second</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">state</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gemma</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">2b</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">it</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">GGUF</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ollama run modelscope.cn</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Shanghai_AI_Laboratory</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">internlm2_5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">7b</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">chat</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">gguf</span></span></code></pre></div><h2 id="配置定制" tabindex="-1">配置定制 <a class="header-anchor" href="#配置定制" aria-label="Permalink to &quot;配置定制&quot;">​</a></h2><p>Ollama支持加载不同精度的GGUF模型，同时在一个GGUF模型库中，一般也会有不同精度的模型文件存在，例如Q3_K_M, Q4_K_M, Q5_K等等，入下图所示：</p><p>一个模型repo下的不同GGUF文件，对应的是不同量化精度与量化方法。默认情况下，如果模型repo里有Q4_K_M版本的话，我们会自动拉取并使用该版本，在推理精度以及推理速度，资源消耗之间做一个较好的均衡。如果没有该版本，我们会选择合适的其他版本。</p><p>此外，您也可以显式配置来指定想要使用的版本。例如</p><div class="language-powershell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">powershell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ollama run modelscope.cn</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Qwen</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Qwen2.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">3B</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Instruct</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">GGUF:Q3_K_M</span></span></code></pre></div><p>这里命令行最后的<code>:Q3_K_M</code>选项，就指定了使用Q3_K_M精度的GGUF模型版本，这个选项大小写不敏感，也就是说，无论是<code>:Q3_K_M</code>，还是<code>:q3_k_m</code>，都是使用模型repo里的&quot;qwen2.5-3b-instruct-q3_k_m.gguf&quot; 这个模型文件。当然，您也可以直接指定模型文件的全称，这同样是支持的：</p><div class="language-powershell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">powershell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ollama run modelscope.cn</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Qwen</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Qwen2.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">3B</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Instruct</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">GGUF:qwen2.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">3b</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">instruct</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">q3_k_m.gguf</span></span></code></pre></div><h3 id="视觉多模态模型使用" tabindex="-1">视觉多模态模型使用 <a class="header-anchor" href="#视觉多模态模型使用" aria-label="Permalink to &quot;视觉多模态模型使用&quot;">​</a></h3><p>除了常见的LLM以外，这种使用方式也支持了包括Llama3.2-Vision在内的视觉多模态模型。这类模型需要确保使用Ollama 0.4.0以上的版本。 例如：</p><div class="language-powershell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">powershell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ollama run modelscope.cn</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">AI</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ModelScope</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Llama</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3.2</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">11B</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Vision</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Instruct</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">GGUF</span></span></code></pre></div><div class="tip custom-block"><p class="custom-block-title">小贴士</p><p>最重要的不是早起本身，而是培养规律的作息习惯。</p></div>`,12)]))}const c=a(t,[["render",p]]);export{g as __pageData,c as default};
