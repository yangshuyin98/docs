import{_ as i,c as t,o as e,ag as n,j as s}from"./chunks/framework.oP1PDRBo.js";const k=JSON.parse('{"title":"Qwen2.5-Coder-7B-Instruct-AWQ模型","description":"","frontmatter":{},"headers":[],"relativePath":"ollama/vllm/Ubuntu Qwen2.5API.md","filePath":"ollama/vllm/Ubuntu Qwen2.5API.md","lastUpdated":1744448235000}'),p={name:"ollama/vllm/Ubuntu Qwen2.5API.md"};function l(o,a,h,d,r,c){return e(),t("div",null,a[0]||(a[0]=[n(`<h1 id="qwen2-5-coder-7b-instruct-awq模型" tabindex="-1">Qwen2.5-Coder-7B-Instruct-AWQ模型 <a class="header-anchor" href="#qwen2-5-coder-7b-instruct-awq模型" aria-label="Permalink to &quot;Qwen2.5-Coder-7B-Instruct-AWQ模型&quot;">​</a></h1><p>以下是使用VLLM部署Qwen2.5-Coder-7B-Instruct-AWQ模型并通过PyCharm调用其API的分步指南：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">确保模型目录包含：</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">model.safetensors</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # 必须存在</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">config.json</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 必须包含&quot;quantization_config&quot;字段</span></span></code></pre></div><p>硬件要求：</p><table tabindex="0"><thead><tr><th style="text-align:left;">组件</th><th style="text-align:left;">最低要求</th><th style="text-align:left;">推荐配置</th></tr></thead><tbody><tr><td style="text-align:left;">GPU</td><td style="text-align:left;">NVIDIA GTX 1080 (8GB)</td><td style="text-align:left;">RTX 3090 (24GB)</td></tr><tr><td style="text-align:left;">RAM</td><td style="text-align:left;">16GB</td><td style="text-align:left;">32GB+</td></tr><tr><td style="text-align:left;">VRAM</td><td style="text-align:left;">8GB</td><td style="text-align:left;">16GB+</td></tr></tbody></table><p>性能优化参数：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 启动服务器时添加这些参数</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">--tensor-parallel-size</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # 多GPU并行</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">--gpu-memory-utilization</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.9</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # 显存利用率</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">--max-num-seqs</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 64</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 提高并发处理量</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">--enforce-eager</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">          # 小显存模式</span></span></code></pre></div><h2 id="_1-环境清理与准备" tabindex="-1">1.环境清理与准备 <a class="header-anchor" href="#_1-环境清理与准备" aria-label="Permalink to &quot;1.环境清理与准备&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># 创建纯净虚拟环境（避免旧依赖干扰）</span></span>
<span class="line"><span>python -m venv vllm_env</span></span></code></pre></div><h2 id="_2-确认虚拟环境激活" tabindex="-1">2.确认虚拟环境激活 <a class="header-anchor" href="#_2-确认虚拟环境激活" aria-label="Permalink to &quot;2.确认虚拟环境激活&quot;">​</a></h2><div class="language-Linux vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">Linux</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>source vllm_env/bin/activate</span></span></code></pre></div><h2 id="_3-安装依赖" tabindex="-1">3.安装依赖 <a class="header-anchor" href="#_3-安装依赖" aria-label="Permalink to &quot;3.安装依赖&quot;">​</a></h2><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># 安装vLLM 0.8.2（需与numpy&lt;2.0兼容）</span></span>
<span class="line"><span>pip install vllm==0.8.2</span></span></code></pre></div><p>安装必要的库，比如vllm和openai。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip install  openai</span></span></code></pre></div><h2 id="_4-启动vllm-api服务器" tabindex="-1">4.启动VLLM API服务器 <a class="header-anchor" href="#_4-启动vllm-api服务器" aria-label="Permalink to &quot;4.启动VLLM API服务器&quot;">​</a></h2><h4 id="在ubuntu终端启动api服务器" tabindex="-1">在Ubuntu终端启动API服务器： <a class="header-anchor" href="#在ubuntu终端启动api服务器" aria-label="Permalink to &quot;在Ubuntu终端启动API服务器：&quot;">​</a></h4><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>python -m vllm.entrypoints.openai.api_server --model /home/modern/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ --quantization awq --host 0.0.0.0 --port 8000</span></span></code></pre></div><p>参数说明‌： --model: 模型路径（需替换为实际路径）。 --quantization awq: 启用AWQ量化加速。</p><p>--allowed-origins &#39;[&quot;*&quot;]&#39; # JSON 数组格式 允许所有域名</p><p>--host 0.0.0.0: 允许外部访问。 --port 8000: 指定API端口。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>python -m vllm.entrypoints.openai.api_server --model /home/modern/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ --quantization awq_marlin --host 0.0.0.0 --port 8000 --generation-config vllm  --served-model-name MyCoder</span></span></code></pre></div><p>启动时指定：</p><p>自定义API端点：--served-model-name MyCoder</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>--served-model-name MyCoder  # 自定义模型名称</span></span></code></pre></div><p>--served-model-name MyCoder \\ # 与API请求中的model参数一致</p><p>--generation-config vllm</p><p>--quantization awq_marlin 该模型在运行时可转换为awq_marlin。使用awq_marlin内核。</p>`,28),s("p",{"reward,":"","embed,":"","generate,":"","score,":"",classify:""},null,-1),s("p",null,"此模型支持多种任务：{“转发”、“嵌入”、“生成”、“评分”、“分类”}。默认为“生成”。",-1),s("p",{"repetition_penalty:":"","1.1,":"","temperature:":"","0.7,":"","top_k:":"","20,":"","top_p:":"","0.8":""},"Using default chat sampling params from model:",-1),n(`<p>‘重复惩罚’：1.1， “温度”：0.7， &#39;top_k&#39;：20， &#39;top_p&#39;：0.8</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>INFO 03-28 08:19:16 [api_server.py:1028] Starting vLLM API server on http://0.0.0.0:8000</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:26] Available routes are:</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /openapi.json, Methods: GET, HEAD</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /docs, Methods: GET, HEAD</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /docs/oauth2-redirect, Methods: GET, HEAD</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /redoc, Methods: GET, HEAD</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /health, Methods: GET</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /load, Methods: GET</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /ping, Methods: POST, GET</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /tokenize, Methods: POST</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /detokenize, Methods: POST</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /v1/models, Methods: GET</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /version, Methods: GET</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /v1/chat/completions, Methods: POST</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /v1/completions, Methods: POST</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /v1/embeddings, Methods: POST</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /pooling, Methods: POST</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /score, Methods: POST</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /v1/score, Methods: POST</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /v1/audio/transcriptions, Methods: POST</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /rerank, Methods: POST</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /v1/rerank, Methods: POST</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /v2/rerank, Methods: POST</span></span>
<span class="line"><span>INFO 03-28 08:19:16 [launcher.py:34] Route: /invocations, Methods: POST</span></span>
<span class="line"><span>INFO:     Started server process [709]</span></span>
<span class="line"><span>INFO:     Waiting for application startup.</span></span>
<span class="line"><span>INFO:     Application startup complete.</span></span></code></pre></div><p>点击运行按钮，控制台输出 <code>INFO: Application startup complete</code> 表示启动成功。</p><h2 id="_5-编写pycharm中创建客户端文件" tabindex="-1">5.编写PyCharm中创建客户端文件： <a class="header-anchor" href="#_5-编写pycharm中创建客户端文件" aria-label="Permalink to &quot;5.编写PyCharm中创建客户端文件：&quot;">​</a></h2><h3 id="配置-pycharm‌" tabindex="-1">配置 PyCharm‌ <a class="header-anchor" href="#配置-pycharm‌" aria-label="Permalink to &quot;配置 PyCharm‌&quot;">​</a></h3><ol><li>在 PyCharm 中打开项目，进入 <code>File &gt; Settings &gt; Project: &lt;项目名&gt; &gt; Python Interpreter</code>。</li><li>点击齿轮图标选择 <code>Add Interpreter &gt; Existing</code>。</li><li>找到虚拟环境的 Python 解释器路径（例如 <code>~/vllm_env/bin/python</code>）。</li></ol><h4 id="网络配置‌" tabindex="-1">网络配置‌： <a class="header-anchor" href="#网络配置‌" aria-label="Permalink to &quot;网络配置‌：&quot;">​</a></h4><p>若VLLM服务部署在远程服务器，需将 localhost 替换为服务器IP，并确保防火墙开放端口（如8000）。</p><p>错误排查‌：检查VLLM日志（vllm.log）是否有启动错误。 测试API连通性：curl <a href="http://172.18.15.27:8000/v1/models" target="_blank" rel="noreferrer">http://172.18.15.27:8000/v1/models</a></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>{&quot;object&quot;:&quot;list&quot;,&quot;data&quot;:[{&quot;id&quot;:&quot;/home/modern/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&quot;,&quot;object&quot;:&quot;model&quot;,&quot;created&quot;:1743128501,&quot;owned_by&quot;:&quot;vllm&quot;,&quot;root&quot;:&quot;/home/modern/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&quot;,&quot;parent&quot;:null,&quot;max_model_len&quot;:32768,&quot;permission&quot;:[{&quot;id&quot;:&quot;modelperm-e3b9d839f2de483187de047503f31f1f&quot;,&quot;object&quot;:&quot;model_permission&quot;,&quot;created&quot;:1743128501,&quot;allow_create_engine&quot;:false,&quot;allow_sampling&quot;:true,&quot;allow_logprobs&quot;:true,&quot;allow_search_indices&quot;:false,&quot;allow_view&quot;:true,&quot;allow_fine_tuning&quot;:false,&quot;organization&quot;:&quot;*&quot;,&quot;group&quot;:null,&quot;is_blocking&quot;:false}]}]}</span></span></code></pre></div><p>提供服务端的完整启动命令</p><p>curl <a href="http://172.18.15.27:8000/v1/models" target="_blank" rel="noreferrer">http://172.18.15.27:8000/v1/models</a> 的输出结果</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>{&quot;object&quot;:&quot;list&quot;,&quot;data&quot;:[{&quot;id&quot;:&quot;MyCoder&quot;,&quot;object&quot;:&quot;model&quot;,&quot;created&quot;:1743145239,&quot;owned_by&quot;:&quot;vllm&quot;,&quot;root&quot;:&quot;/home/modern/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&quot;,&quot;parent&quot;:null,&quot;max_model_len&quot;:32768,&quot;permission&quot;:[{&quot;id&quot;:&quot;modelperm-02048ca2299f4ce89ac9a57253c52d57&quot;,&quot;object&quot;:&quot;model_permission&quot;,&quot;created&quot;:1743145239,&quot;allow_create_engine&quot;:false,&quot;allow_sampling&quot;:true,&quot;allow_logprobs&quot;:true,&quot;allow_search_indices&quot;:false,&quot;allow_view&quot;:true,&quot;allow_fine_tuning&quot;:false,&quot;organization&quot;:&quot;*&quot;,&quot;group&quot;:null,&quot;is_blocking&quot;:false}]}]}</span></span></code></pre></div><p>扩展功能‌</p><p>调整生成参数‌： 修改API请求中的 temperature, top_p, max_tokens 等参数控制生成结果。</p><p><code>top_p</code>、<code>top_k</code> 以及 <code>temperature</code> 都是与大语言模型生成文本过程相关的采样参数。以下是对每个参数的解释：</p><p>参数组合策略 保守生成（代码/事实类）： top_p=0.8 top_k=30 temperature=0.3 创意生成（故事/文案类）： top_p=0.95 top_k=0 # 禁用 top_k temperature=1.0 平衡模式（通用对话）： top_p=0.9 top_k=50 temperature=0.7</p><p>注意：与 top_k 二选一使</p><ol><li><p><strong><code>repetition_penalty</code>:</strong></p><ul><li>中文含义：重复惩罚</li><li>作用：用于控制模型在生成文本时对重复使用相同词汇或短语的限制。</li><li>解释：较高的 <code>repetition_penalty</code> 值会减少模型重复相似内容的可能性，从而降低结果中出现重复词语的风险。</li></ul></li><li><p><strong><code>temperature</code>:</strong></p><ul><li>中文含义：温度</li><li>作用：控制生成文本的随机性和创造性程度。</li><li>解释： <ul><li>温度较高（例如 0.9）时，模型会更加随机和富有创造力，可能会跳脱出常规思维进行创作。但若温度过高（如接近或超过 1），可能导致输出的内容不够连贯，甚至出现怪异的结果。</li><li>温度较低（例如 0.3）时，模型的决策更加保守和确定性，倾向于选择较为常见和合理的表达方式，输出更加稳定和收敛。</li></ul></li></ul></li><li><p><strong><code>top_k</code>:</strong></p><ul><li>中文含义：限制候选词数量，防止生成重复内容</li><li>推荐值域：20~100</li><li>作用：在生成下一个词时，从所有可能候选中随机选择前 k 个概率最高的词汇。</li><li>解释： <ul><li>较小的 <code>top_k</code> 值（比如 10）会使模型选择范围较窄，输出更加集中和保守。</li><li>较大的 <code>top_k</code> 值（比如 50 或更高）会让模型有更多选择余地，从而生成更加多样化且意想不到的结果。</li></ul></li></ul></li><li><p><strong><code>top_p</code>:</strong></p><ul><li>中文含义：核采样（概率累积阈值）</li><li>推荐值域：0.5~0.95</li><li>作用：基于词汇的概率累积和，只在所有候选中选择概率累积和超过 <code>p%</code> 的范围内的词汇。</li><li>解释： <ul><li>较小的 <code>top_p</code> 值（例如 0.2）会极大地限制生成的内容，减少结果的多样性。</li><li>较大的 <code>top_p</code> 值（例如 0.8 或更高）则允许更多的词汇选择，增加生成内容的新意和不可预测性。</li></ul></li></ul></li></ol><hr><p>总结来说：</p><ul><li><strong>温度 (<code>temperature</code>) 是创意与稳定性的平衡点。</strong></li><li><strong>顶部 k (<code>top_k</code>) 和 顶部 p (<code>top_p</code>) 都是控制多样性的方式，分别从数量上的-top k 个或概率累积的-top p% 值选择候选项。</strong></li><li><strong>重复惩罚 (<code>repetition_penalty</code>) 则是用来防止模型在生成文本时过于重复之前的内容。</strong></li></ul><p>这些参数可以根据具体需求进行调整，从而影响模型输出的结果风格和质量。</p><h4 id="编写api调用代码‌" tabindex="-1">编写API调用代码‌ <a class="header-anchor" href="#编写api调用代码‌" aria-label="Permalink to &quot;编写API调用代码‌&quot;">​</a></h4><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>import requests</span></span>
<span class="line"><span>import json</span></span>
<span class="line"><span></span></span>
<span class="line"><span>url = &quot;http://172.18.15.27:8000/v1/completions&quot;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>headers = {</span></span>
<span class="line"><span>    &quot;Content-Type&quot;: &quot;application/json&quot;</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span></span></span>
<span class="line"><span>data = {</span></span>
<span class="line"><span>    &quot;model&quot;: &quot;/home/modern/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&quot;,</span></span>
<span class="line"><span>    &quot;prompt&quot;: &quot;Hello, my name is&quot;,</span></span>
<span class="line"><span>    &quot;max_tokens&quot;: 9000,</span></span>
<span class="line"><span>    &quot;temperature&quot;: 0.7,</span></span>
<span class="line"><span>    &quot;top_k&quot;:50,</span></span>
<span class="line"><span>    &quot;top_p&quot;:0.8</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span></span></span>
<span class="line"><span>response = requests.post(url, headers=headers, data=json.dumps(data))</span></span>
<span class="line"><span>print(&quot;Status Code:&quot;, response.status_code)</span></span>
<span class="line"><span>print(&quot;Response JSON:\\n&quot;, json.dumps(response.json(), indent=2))</span></span></code></pre></div><p>输出内容为：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>D:\\WEB\\PycharmProjects\\venv\\Scripts\\python.exe D:\\WEB\\PycharmProjects\\pythonProject1\\markdown\\openai_client.py </span></span>
<span class="line"><span>Status Code: 200</span></span>
<span class="line"><span>Response JSON:</span></span>
<span class="line"><span> {</span></span>
<span class="line"><span>  &quot;id&quot;: &quot;cmpl-fda6dd053c2f421d8fec68ae3463b96f&quot;,</span></span>
<span class="line"><span>  &quot;object&quot;: &quot;text_completion&quot;,</span></span>
<span class="line"><span>  &quot;created&quot;: 1743129660,</span></span>
<span class="line"><span>  &quot;model&quot;: &quot;/home/modern/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&quot;,</span></span>
<span class="line"><span>  &quot;choices&quot;: [</span></span>
<span class="line"><span>    {</span></span>
<span class="line"><span>      &quot;index&quot;: 0,</span></span>
<span class="line"><span>      &quot;text&quot;: &quot; [Your Name] and I am a [Your Profession]. I am writing to you today because I am interested in learning more about [Your Area of Interest]. I believe that your expertise and knowledge in this field would be invaluable to me and I am hoping that you could share some of your insights and advice with me. I am open to any questions you may have and I am excited to learn more about this topic. Thank you for your time and consideration. Sincerely, [Your Name]&quot;,</span></span>
<span class="line"><span>      &quot;logprobs&quot;: null,</span></span>
<span class="line"><span>      &quot;finish_reason&quot;: &quot;stop&quot;,</span></span>
<span class="line"><span>      &quot;stop_reason&quot;: null,</span></span>
<span class="line"><span>      &quot;prompt_logprobs&quot;: null</span></span>
<span class="line"><span>    }</span></span>
<span class="line"><span>  ],</span></span>
<span class="line"><span>  &quot;usage&quot;: {</span></span>
<span class="line"><span>    &quot;prompt_tokens&quot;: 5,</span></span>
<span class="line"><span>    &quot;total_tokens&quot;: 105,</span></span>
<span class="line"><span>    &quot;completion_tokens&quot;: 100,</span></span>
<span class="line"><span>    &quot;prompt_tokens_details&quot;: null</span></span>
<span class="line"><span>  }</span></span>
<span class="line"><span>}</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Process finished with exit code 0</span></span></code></pre></div><h4 id="编写api调用代码‌——stream" tabindex="-1">编写API调用代码‌——stream <a class="header-anchor" href="#编写api调用代码‌——stream" aria-label="Permalink to &quot;编写API调用代码‌——stream&quot;">​</a></h4><p>流式输出（streaming response），这样客户端可以逐步接收生成的内容，而不是等待整个生成完成。</p><p>设置 &quot;stream&quot;: True 后，API会持续返回结果（逐token输出），适用于需要实时反馈的场景。</p><p>用Python代码调用这个API。需要用到openai库，并配置base_url指向本地运行的VLLM服务器，以及正确的API密钥（虽然VLLM可能不需要，但为了兼容性可能需要设置一个虚拟的）。同时，处理流式响应需要在客户端循环读取返回的数据块，并拼接结果。</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># modern/scripts/openai_client.py</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 初始化客户端</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    base_url</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;http://172.18.15.27:8000/v1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 本地访问地址</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    api_key</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;EMPTY&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># VLLM默认不需要鉴权</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    max_retries</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 新增HTTP重试</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    timeout</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10.0</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # 单独设置TCP连接超时</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 流式请求</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">stream </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.chat.completions.create(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;/home/modern/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    messages</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">   	 	{</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;system&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;You are a helpful assistant.&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">},</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    	{</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;如何使用你的编程，列举几个例子&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ],</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    stream</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 启用流式传输</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    temperature</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    top_p</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.9</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 新增参数</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    max_tokens</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4096</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;[系统] 开始生成回答...&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">flush</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 处理流式响应</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> chunk </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> stream:  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 每个chunk中的choices是一个包含至少一个对象的列表</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    content </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> chunk.choices[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].delta.content </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#每个choice对象有delta属性，delta里又有content。</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> content:</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(content, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">end</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">flush</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><p>VLLM是否支持流式传输？是的，VLLM的OpenAI API接口支持stream参数，当设置为True时，会以流式返回结果，每个token逐个发送。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># modern/scripts/openai_client.py</span></span>
<span class="line"><span>from openai import OpenAI  # 导入OpenAI SDK客户端（用于连接大模型服务）</span></span>
<span class="line"><span># 初始化OpenAI客户端配置</span></span>
<span class="line"><span>client = OpenAI(</span></span>
<span class="line"><span>    base_url=&quot;http://172.18.15.27:8000/v1&quot;,  # 配置本地模型服务的API端点地址</span></span>
<span class="line"><span>    api_key=&quot;EMPTY&quot;              # API密钥留空（可能用于内部测试环境）</span></span>
<span class="line"><span>)</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 定义流式响应处理函数</span></span>
<span class="line"><span>def stream_response(prompt):</span></span>
<span class="line"><span>    # 创建流式响应生成器（保持长连接实时获取输出）</span></span>
<span class="line"><span>    stream = client.chat.completions.create(</span></span>
<span class="line"><span>        model=&quot;/home/modern/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&quot;,  # 指定本地模型路径</span></span>
<span class="line"><span>        messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],         # 构建对话消息结构</span></span>
<span class="line"><span>        stream=True,                             # 启用流式返回模式</span></span>
<span class="line"><span>        temperature=0.3,                      # 设置随机性参数（低值更确定）</span></span>
<span class="line"><span>        max_tokens=4096,  # 优化6：调整为模型支持的最大值（避免报错）</span></span>
<span class="line"><span>        top_p=0.2,  # 新增参数（例如 0.2）会极大地限制生成的内容，减少结果的多样性</span></span>
<span class="line"><span>        timeout=30.0,  # 优化7：添加超时控制</span></span>
<span class="line"><span>    )</span></span>
<span class="line"><span>    print(&quot;[开始生成回答...]&quot;, end=&quot;  &quot;)</span></span>
<span class="line"><span>    # 逐块处理流式返回内容</span></span>
<span class="line"><span>    for chunk in stream:</span></span>
<span class="line"><span>        # 提取当前块的文本内容（delta表示增量更新）</span></span>
<span class="line"><span>        content = chunk.choices[0].delta.content</span></span>
<span class="line"><span>        if content:                               # 过滤空内容并实时输出</span></span>
<span class="line"><span>            print(content, end=&quot;&quot;, flush=True)         # 禁用换行+强制立即输出</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>if __name__ == &quot;__main__&quot;:</span></span>
<span class="line"><span>    stream_response(&quot;写一个Python正则表达式验证邮箱格式&quot;)    # 调用流式响应函数并传入测试提示词</span></span></code></pre></div><p>通过以上步骤，您可以在PyCharm中实现：1）低延迟的代码生成 2）实时流式输出 3）多轮对话支持。实际部署时建议使用反向代理（如Nginx）增强API安全性。</p><h5 id="openai-client客户端代码存放位置" tabindex="-1">openai_client客户端代码存放位置 <a class="header-anchor" href="#openai-client客户端代码存放位置" aria-label="Permalink to &quot;openai_client客户端代码存放位置&quot;">​</a></h5><p>(1) 推荐存放路径：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>modern/</span></span>
<span class="line"><span>├── scripts/</span></span>
<span class="line"><span>│   ├── qwen_inference.py  # API服务器启动脚本</span></span>
<span class="line"><span>│   └── openai_client.py   # 新建的PyCharm客户端代码 👈 推荐位置</span></span></code></pre></div><h2 id="_6-跨系统访问配置" tabindex="-1">6. 跨系统访问配置 <a class="header-anchor" href="#_6-跨系统访问配置" aria-label="Permalink to &quot;6. 跨系统访问配置&quot;">​</a></h2><p>跨系统访问规则：</p><table tabindex="0"><thead><tr><th style="text-align:left;">场景</th><th style="text-align:left;">服务器位置</th><th style="text-align:left;">客户端位置</th><th style="text-align:left;">配置要点</th></tr></thead><tbody><tr><td style="text-align:left;">最佳实践</td><td style="text-align:left;">Ubuntu 20.04</td><td style="text-align:left;">Ubuntu 20.04</td><td style="text-align:left;"><code>base_url=&quot;http://localhost:8000/v1&quot;</code></td></tr><tr><td style="text-align:left;">跨系统访问</td><td style="text-align:left;">Ubuntu 20.04</td><td style="text-align:left;">Windows 10</td><td style="text-align:left;">需修改为Ubuntu的IP：<code>base_url=&quot;http://[Ubuntu_IP]:8000/v1&quot;</code></td></tr></tbody></table><h3 id="windows访问ubuntu服务的步骤" tabindex="-1">Windows访问Ubuntu服务的步骤： <a class="header-anchor" href="#windows访问ubuntu服务的步骤" aria-label="Permalink to &quot;Windows访问Ubuntu服务的步骤：&quot;">​</a></h3><ol><li><p>在Ubuntu终端查看IP地址：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ip addr show eth0 | grep &quot;inet &quot;</span></span>
<span class="line"><span>ip a  # 查看ens33或eth0的inet地址</span></span></code></pre></div><p>inet 172.21.136.206/20 brd 172.21.143.255 scope global eth0</p></li><li><p>修改客户端连接地址：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">base_url</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;http://172.21.136.206:8000/v1&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # 替换为实际Ubuntu IP</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    base_url</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;http://172.21.136.206:8000/v1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 替换此处为Ubuntu的实际IP</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    api_key</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;EMPTY&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div></li><li><p>开放Ubuntu防火墙：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> ufw</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> allow</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> 8000/tcp</span></span></code></pre></div></li></ol><h4 id="网络连通性验证" tabindex="-1">网络连通性验证 <a class="header-anchor" href="#网络连通性验证" aria-label="Permalink to &quot;网络连通性验证&quot;">​</a></h4><p>在Windows命令提示符中执行：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">ping</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 172.21.136.206</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">          # 测试基础连通性</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">telnet</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 172.21.136.206</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 8000</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">   # 测试端口可达性（需开启Telnet客户端功能）</span></span></code></pre></div><p>预期结果：</p><ul><li>ping成功：表示网络层可达</li><li>telnet连接成功：表示VLLM的8000端口已开放</li></ul><p>补充说明</p><ol><li><p>地址类型：<code>172.21.136.206</code> 是私有地址（B类私有地址范围：<code>172.16.0.0~172.31.255.255</code>），适用于内网环境</p></li><li><p>WSL2用户注意：如果您使用的是WSL2，需在Windows中执行以下命令启用端口转发：</p><div class="language-powershell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">powershell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">netsh interface portproxy add v4tov4 listenport</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">8000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> listenaddress</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> connectport</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">8000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> connectaddress</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">172.21</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">136.206</span></span></code></pre></div></li></ol><hr><p>您现在可以直接在Windows客户端的代码中使用 <code>172.21.136.206:8000</code> 访问Ubuntu的VLLM服务。</p><h3 id="跨系统访问配置-win10——ip" tabindex="-1">跨系统访问配置-win10——IP <a class="header-anchor" href="#跨系统访问配置-win10——ip" aria-label="Permalink to &quot;跨系统访问配置-win10——IP&quot;">​</a></h3><h4 id="获取宿主机的局域网ip" tabindex="-1">获取宿主机的局域网IP <a class="header-anchor" href="#获取宿主机的局域网ip" aria-label="Permalink to &quot;获取宿主机的局域网IP&quot;">​</a></h4><p>启动服务时需绑定到 <code>0.0.0.0</code>（所有网络接口），而非默认的 <code>127.0.0.1</code>。 修正命令：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># 在WSL中执行（确保添加--host参数）</span></span>
<span class="line"><span>CUDA_VISIBLE_DEVICES=0 vllm serve /path/to/model \\</span></span>
<span class="line"><span>  --host 0.0.0.0 \\  # 关键！允许局域网访问</span></span>
<span class="line"><span>  --port 8102 \\</span></span>
<span class="line"><span>  --max-model-len 2048 \\</span></span>
<span class="line"><span>  --quantization awq</span></span></code></pre></div><p>在Windows 10（宿主机）中按 <code>Win+R</code> 输入 <code>cmd</code> 执行：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>ipconfig | findstr &quot;IPv4&quot;</span></span></code></pre></div><p>记录输出中的IPv4地址（如 172.18.15.27）。</p><h4 id="配置windows防火墙" tabindex="-1">配置Windows防火墙 <a class="header-anchor" href="#配置windows防火墙" aria-label="Permalink to &quot;配置Windows防火墙&quot;">​</a></h4><h5 id="_1-开放端口" tabindex="-1">1. 开放端口 <a class="header-anchor" href="#_1-开放端口" aria-label="Permalink to &quot;1. 开放端口&quot;">​</a></h5><ol><li>打开 控制面板 → 系统和安全 → Windows Defender 防火墙 → 高级设置</li><li>右键 入站规则 → 新建规则 → 端口 → TCP 8102 → 允许连接 → 完成</li></ol><h5 id="_2-验证防火墙规则" tabindex="-1">2. 验证防火墙规则 <a class="header-anchor" href="#_2-验证防火墙规则" aria-label="Permalink to &quot;2. 验证防火墙规则&quot;">​</a></h5><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># 以管理员身份运行PowerShell</span></span>
<span class="line"><span>Get-NetFirewallRule | Where-Object { $_.LocalPort -eq 8102 }</span></span></code></pre></div><p>确保状态为 <code>Enabled</code> 且动作为 <code>Allow</code></p><h2 id="_7-安全加固-可选" tabindex="-1">7.安全加固（可选） <a class="header-anchor" href="#_7-安全加固-可选" aria-label="Permalink to &quot;7.安全加固（可选）&quot;">​</a></h2><h4 id="_1-api密钥验证" tabindex="-1">1. API密钥验证 <a class="header-anchor" href="#_1-api密钥验证" aria-label="Permalink to &quot;1. API密钥验证&quot;">​</a></h4><p>启动服务时添加 <code>--api-key</code> 参数：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>vllm serve ... --api-key YOUR_SECRET_KEY</span></span></code></pre></div><p>调用时需携带密钥：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>from openai import OpenAI</span></span>
<span class="line"><span>client = OpenAI(</span></span>
<span class="line"><span>    base_url=&quot;http://192.168.1.100:8102/v1&quot;,</span></span>
<span class="line"><span>    api_key=&quot;YOUR_SECRET_KEY&quot;</span></span>
<span class="line"><span>)</span></span></code></pre></div><h4 id="_2-ip白名单限制" tabindex="-1">2. IP白名单限制 <a class="header-anchor" href="#_2-ip白名单限制" aria-label="Permalink to &quot;2. IP白名单限制&quot;">​</a></h4><p>在Windows防火墙中配置 入站规则 → 作用域 → 仅允许下列IP地址，添加允许的局域网IP段（如 <code>192.168.1.0/24</code>）</p><h2 id="_8-跨设备调用示例代码" tabindex="-1">8.跨设备调用示例代码 <a class="header-anchor" href="#_8-跨设备调用示例代码" aria-label="Permalink to &quot;8.跨设备调用示例代码&quot;">​</a></h2><h4 id="python客户端-任意设备" tabindex="-1">Python客户端（任意设备）： <a class="header-anchor" href="#python客户端-任意设备" aria-label="Permalink to &quot;Python客户端（任意设备）：&quot;">​</a></h4><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>from openai import OpenAI</span></span>
<span class="line"><span># 初始化客户端</span></span>
<span class="line"><span>client = OpenAI(</span></span>
<span class="line"><span>    base_url=&quot;http://172.18.15.27:8000/v1&quot;,</span></span>
<span class="line"><span>    api_key=&quot;EMPTY&quot;  # 若未设置--api-key则留空</span></span>
<span class="line"><span>)</span></span>
<span class="line"><span></span></span>
<span class="line"><span>response = client.completions.create(</span></span>
<span class="line"><span>    model=&quot;/home/modern/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&quot;,</span></span>
<span class="line"><span>    prompt=&quot;请写一首关于春天的诗&quot;,</span></span>
<span class="line"><span>    列举使用编程功能的提示词</span></span>
<span class="line"><span>    max_tokens=4096,</span></span>
<span class="line"><span>    top_p=0.1,  # 新增参数</span></span>
<span class="line"><span>    temperature=0.7</span></span>
<span class="line"><span>)</span></span>
<span class="line"><span>print(response.choices[0].text)</span></span></code></pre></div><h4 id="javascript-浏览器" tabindex="-1">JavaScript（浏览器）： <a class="header-anchor" href="#javascript-浏览器" aria-label="Permalink to &quot;JavaScript（浏览器）：&quot;">​</a></h4><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>// 直接在浏览器控制台测试</span></span>
<span class="line"><span>fetch(&#39;http://172.18.15.27:8000/v1/completions&#39;, {</span></span>
<span class="line"><span>  method: &#39;POST&#39;,</span></span>
<span class="line"><span>  headers: { &#39;Content-Type&#39;: &#39;application/json&#39; },</span></span>
<span class="line"><span>  body: JSON.stringify({</span></span>
<span class="line"><span>    model: &quot;MyCoder&quot;,</span></span>
<span class="line"><span>    prompt: &#39;如何学习机器学习？&#39;,</span></span>
<span class="line"><span>    max_tokens: 100</span></span>
<span class="line"><span>  })</span></span>
<span class="line"><span>})</span></span>
<span class="line"><span>.then(response =&gt; response.json())</span></span>
<span class="line"><span>.then(data =&gt; console.log(data.choices[0].text));</span></span></code></pre></div><h2 id="_9-编写api调用代码‌" tabindex="-1">9.编写API调用代码‌ <a class="header-anchor" href="#_9-编写api调用代码‌" aria-label="Permalink to &quot;9.编写API调用代码‌&quot;">​</a></h2><p>使用Chat模式‌： 如果模型支持对话，可改用 /v1/chat/completions 接口：</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">API_URL</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;http://172.18.15.27:8000/v1&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">data </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">    &quot;messages&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [{</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: prompt}]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre></div><p>通过以上步骤，您可以在PyCharm中轻松调用VLLM部署的模型API。如果需要更复杂的交互逻辑（如对话历史管理），可进一步封装API请求。</p>`,82)]))}const g=i(p,[["render",l]]);export{k as __pageData,g as default};
