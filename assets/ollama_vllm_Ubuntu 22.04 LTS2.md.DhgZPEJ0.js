import{_ as a,c as n,o as p,ag as e}from"./chunks/framework.oP1PDRBo.js";const u=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"ollama/vllm/Ubuntu 22.04 LTS2.md","filePath":"ollama/vllm/Ubuntu 22.04 LTS2.md","lastUpdated":1744277030000}'),i={name:"ollama/vllm/Ubuntu 22.04 LTS2.md"};function t(l,s,o,h,c,d){return p(),n("div",null,s[0]||(s[0]=[e(`<p>我们介绍了我们的第一代推理模型，DeepSeek-R1-Zero和DeepSeek-R1。DeepSeek-R1-Zero是一种通过大规模强化学习（RL）训练的模型，没有监督微调（SFT）作为初步步骤，在推理方面表现出卓越的性能。有了强化学习，DeepSeek-R1-Zero自然会出现许多强大而有趣的推理行为。然而，DeepSeek-R1-Zero遇到了无休止的重复、可读性差和语言混合等挑战。为了解决这些问题并进一步提高推理性能，我们引入了DeepSeek-R1，它在RL之前合并了冷启动数据。DeepSeek-R1在数学、代码和推理任务上实现了与OpenAI-o 1相当的性能。为了支持研究社区，我们拥有开源的DeepSeek-R1-Zero、DeepSeek-R1以及基于Llama和Qwen的从DeepSeek-R1中提炼出来的六个密集模型。DeepSeek-R1-Distill-Qwen-32 B在各种基准测试中的表现优于OpenAI-o 1-mini，为密集模型实现了新的最先进的结果。</p><p><strong>后训练：基于基础模型的大规模强化学习</strong></p><p>我们直接将强化学习（RL）应用于基础模型，而不依赖于监督微调（SFT）作为初步步骤。该方法允许模型探索解决复杂问题的思路链（CoT），从而开发出DeepSeek-R1-Zero。DeepSeek-R1-Zero展示了自我验证、反射和生成长CoT等功能，标志着研究界的一个重要里程碑。值得注意的是，这是第一个验证LLM的推理能力可以纯粹通过RL来激励，而不需要SFT的开放式研究。这一突破为该领域的未来发展铺平了道路。</p><p>我们介绍了我们开发DeepSeek-R1的管道。该流水线包括两个RL阶段，旨在发现改进的推理模式并与人类偏好保持一致，以及两个SFT阶段，用作模型的推理和非推理能力的种子。我们相信，这条管道将通过创造更好的模式而使行业受益。</p><p><strong>蒸馏</strong>Distillation**：更小的模型也可以是强大的**</p><p>我们证明了大模型的推理模式可以被提炼成小模型，从而比通过RL在小模型上发现的推理模式具有更好的性能。开源的DeepSeek-R1及其API将使研究社区在未来提取更好的更小的模型。</p><p>使用DeepSeek-R1生成的推理数据，我们微调了几个在研究界广泛使用的密集模型。评估结果表明，提取的较小密度模型在基准测试中表现出色。我们向社区开放了基于Qwen2.5和Llama 3系列的1.5B、7 B、8B、14 B、32 B和70 B检查点的源代码。</p><p>DeepSeek-R1-Zero DeepSeek-R1基于DeepSeek-V3-Base进行训练。有关模型架构的更多详细信息，请参阅<a href="https://github.com/deepseek-ai/DeepSeek-V3" target="_blank" rel="noreferrer">DeepSeek-V3</a>存储库。</p><p>DeepSeek-R1-Distill模型基于开源模型，使用DeepSeek-R1生成的样本进行了微调。我们稍微更改了它们的配置和标记化器。请使用我们的设置来运行这些模型。</p><p>对于我们的所有模型，最大生成长度设置为32，768个令牌。对于需要采样的基准测试，我们使用0.60.60.6的温度，0.950.950.95的top-p值，并为每个查询生成64个响应以估计pass@1。</p><p>你可以在DeepSeek的官方网站上与DeepSeek-R1聊天：，并打开按钮“DeepThink”</p><p>我们还在DeepSeek平台上提供OpenAI兼容的API</p><h2 id="_6-如何在本地运行" tabindex="-1">6.如何在本地运行 <a class="header-anchor" href="#_6-如何在本地运行" aria-label="Permalink to &quot;6.如何在本地运行&quot;">​</a></h2><p>DeepSeek-R1-Distill模型可以以与Qwen或Llama模型相同的方式使用。</p><p>例如，您可以使用vLLM轻松启动服务：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B </span></span>
<span class="line"><span>--tensor-parallel-size 2 </span></span>
<span class="line"><span>--max-model-len 32768 </span></span>
<span class="line"><span>--enforce-eager</span></span></code></pre></div><h3 id="使用建议" tabindex="-1">使用建议 <a class="header-anchor" href="#使用建议" aria-label="Permalink to &quot;使用建议&quot;">​</a></h3><p><strong>我们建议在使用DeepSeek-R1系列模型时遵守以下配置，包括基准测试，以实现预期性能：</strong></p><p>将温度设置在0.5-0.7（推荐0.6）的范围内，以防止无休止的重复或不连贯的输出。</p><p><strong>避免添加系统提示符;所有指令都应包含在用户提示符中。</strong></p><p>对于数学问题，建议在提示中包含一个指令，例如：“请逐步推理，并将您的最终答案放在\\box{}中。”</p><p>在评估模型性能时，建议进行多次测试并对结果进行平均。</p><p>此外，我们观察到DeepSeek-R1系列模型在响应某些查询时倾向于绕过思维模式（即输出“&lt;思考&gt;\\n\\n&lt;/思考&gt;”），这可能会对模型的性能产生不利影响。为了确保模型进行彻底的推理，我们建议强制模型在每个输出的开头使用“&lt;思考&gt;\\n”来启动其响应</p><h4 id="_1-确认windows的nvidia驱动已安装" tabindex="-1">1.<strong>确认Windows的NVIDIA驱动已安装</strong> <a class="header-anchor" href="#_1-确认windows的nvidia驱动已安装" aria-label="Permalink to &quot;1.**确认Windows的NVIDIA驱动已安装**&quot;">​</a></h4><ul><li>在Windows中打开 <strong>NVIDIA控制面板</strong> → 左下角 <strong>系统信息</strong> → 检查驱动版本是否为 <strong>CUDA 12.1+</strong>（需支持WSL GPU）。</li><li>若未安装，前往<a href="https://www.nvidia.com/Download/index.aspx" target="_blank" rel="noreferrer">NVIDIA官网</a>下载最新驱动。</li></ul><h4 id="配置wsl环境" tabindex="-1"><strong>配置WSL环境</strong> <a class="header-anchor" href="#配置wsl环境" aria-label="Permalink to &quot;**配置WSL环境**&quot;">​</a></h4><ul><li><strong>启用WSL</strong>：在PowerShell（管理员模式）中执行：</li></ul><div class="language-powershell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">powershell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">wsl </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">--</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">install </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">d Ubuntu</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">22.04</span></span></code></pre></div><p><strong>安装NVIDIA驱动</strong>：确保Windows已安装适配的NVIDIA驱动（如CUDA 12.1+）</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>正在安装: Ubuntu 22.04 LTS</span></span>
<span class="line"><span>无法从 Microsoft Store 安装 Ubuntu-22.04: 与服务器的连接意外终止</span></span>
<span class="line"><span>正在尝试 Web 下载...</span></span>
<span class="line"><span>正在下载: Ubuntu 22.04 LTS</span></span>
<span class="line"><span>正在安装: Ubuntu 22.04 LTS</span></span>
<span class="line"><span>已安装 Ubuntu 22.04 LTS。</span></span>
<span class="line"><span>正在启动 Ubuntu 22.04 LTS...</span></span>
<span class="line"><span>Installing, this may take a few minutes...</span></span>
<span class="line"><span>Please create a default UNIX user account. The username does not need to match your Windows username.</span></span>
<span class="line"><span>For more information visit: https://aka.ms/wslusers</span></span>
<span class="line"><span>Enter new UNIX username: modern</span></span>
<span class="line"><span>^[[B^[[B^[[B^[[A^[[A</span></span>
<span class="line"><span>New password:</span></span>
<span class="line"><span>Retype new password:</span></span>
<span class="line"><span>passwd: password updated successfully</span></span>
<span class="line"><span>123</span></span>
<span class="line"><span>Installation successful!</span></span>
<span class="line"><span></span></span>
<span class="line"><span>To run a command as administrator (user &quot;root&quot;), use &quot;sudo &lt;command&gt;&quot;.</span></span>
<span class="line"><span>See &quot;man sudo_root&quot; for details.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Welcome to Ubuntu 22.04.2 LTS (GNU/Linux 5.15.167.4-microsoft-standard-WSL2 x86_64)</span></span>
<span class="line"><span></span></span>
<span class="line"><span> * Documentation:  https://help.ubuntu.com</span></span>
<span class="line"><span> * Management:     https://landscape.canonical.com</span></span>
<span class="line"><span> * Support:        https://ubuntu.com/advantage</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>This message is shown once a day. To disable it please create the</span></span>
<span class="line"><span>/home/modern/.hushlogin file.</span></span></code></pre></div><p>检查已安装的发行版列表：</p><div class="language-powershell vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">powershell</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">wsl </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">l </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">v  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 列出所有发行版及其状态（需为 &quot;Running&quot; 或 &quot;Stopped&quot;）</span></span></code></pre></div><p>若列表为空，需重新安装发行版（从 Microsoft Store 下载）。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>  NAME            STATE           VERSION</span></span>
<span class="line"><span>* Ubuntu-22.04    Running         2</span></span></code></pre></div><p>在 WSL 中直接访问 Windows 文件：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>cd /mnt/c/Users/你的用户名  # 例如进入 C 盘用户目录</span></span></code></pre></div><p>彻底卸载命令：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>wsl --unregister Ubuntu-22.04</span></span></code></pre></div><p>进入WSL</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>WSL</span></span></code></pre></div><p><strong>在WSL中安装CUDA Toolkit</strong></p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 进入WSL的Ubuntu终端，依次执行：</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">wget</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> mv</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cuda-wsl-ubuntu.pin</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /etc/apt/preferences.d/cuda-repository-pin-600</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">wget</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://developer.download.nvidia.com/compute/cuda/12.1.1/local_installers/cuda-repo-wsl-ubuntu-12-1-local_12.1.1-1_amd64.deb</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> dpkg</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -i</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cuda-repo-wsl-ubuntu-12-1-local_12.1.1-1_amd64.deb</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cp</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /var/cuda-repo-wsl-ubuntu-12-1-local/cuda-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">*</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">-keyring.gpg</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> /usr/share/keyrings/</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> update</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cuda-toolkit-12-1</span></span></code></pre></div><p><strong>在 WSL2 中安装CUDA</strong>‌</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 安装 CUDA（需先安装 Windows 版 NVIDIA 驱动）</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">wget</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> dpkg</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -i</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cuda-keyring_1.1-1_all.deb</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> update</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -y</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> cuda-toolkit-12-1</span></span></code></pre></div><h2 id="_2-安装python与依赖" tabindex="-1">2.安装Python与依赖** <a class="header-anchor" href="#_2-安装python与依赖" aria-label="Permalink to &quot;2.安装Python与依赖**&quot;">​</a></h2><p>在WSL中安装Python 3.8+，并配置虚拟环境：</p><p><strong>更新系统包列表</strong> 进入 WSL 终端，执行：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>sudo apt update &amp;&amp; sudo apt upgrade -y      #确保系统的软件包列表是最新的</span></span></code></pre></div><h5 id="通过系统包管理器安装-推荐-ubuntu-20-04-用户" tabindex="-1"><strong>通过系统包管理器安装（推荐 Ubuntu 20.04 用户）</strong> <a class="header-anchor" href="#通过系统包管理器安装-推荐-ubuntu-20-04-用户" aria-label="Permalink to &quot;**通过系统包管理器安装（推荐 Ubuntu 20.04 用户）**&quot;">​</a></h5><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -y</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> python3-pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> build-essential</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>sudo apt install ca-certificates apt-transport-https software-properties-common lsb-release -y</span></span></code></pre></div><p><strong>更新软件包列表并安装Python 3.11</strong>：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> update</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">sudo</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> apt</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> python3.11</span></span></code></pre></div><p><strong>验证安装</strong>：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>python3.11 --version</span></span></code></pre></div><p>Python 3.11.0rc1</p><h3 id="安装包管理工具pip" tabindex="-1">安装包管理工具pip <a class="header-anchor" href="#安装包管理工具pip" aria-label="Permalink to &quot;安装包管理工具pip&quot;">​</a></h3><p>pip是Python的包管理工具，用于安装和管理Python包。可以使用以下命令安装pip：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>sudo apt install python3.10-venv  # Ubuntu 22.04 默认 Python 版本为 3.10</span></span>
<span class="line"><span>sudo apt install python3-pip -y</span></span></code></pre></div><p>sudo apt install python3.8 python3-pip</p><p>安装完成后，可以通过以下命令验证pip是否安装成功：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip3 --version</span></span></code></pre></div><p>pip 22.0.2 from /usr/lib/python3/dist-packages/pip (python 3.10)</p><h4 id="三、配置虚拟环境" tabindex="-1"><strong>三、配置虚拟环境</strong> <a class="header-anchor" href="#三、配置虚拟环境" aria-label="Permalink to &quot;**三、配置虚拟环境**&quot;">​</a></h4><p><strong>创建虚拟环境</strong>：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>python3 -m venv venv   # 在项目目录中执行</span></span></code></pre></div><p><strong>激活环境</strong>：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>source venv/bin/activate  # 激活虚拟环境</span></span></code></pre></div><p>提示符变为 <code>(.venv) $</code> 表示激活成功。</p><p><code>提示符变为(venv) modern@DESKTOP-GQCI0GM:~$</code>表示激活成功。</p><p><strong>安装依赖</strong>：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip install -r requirements.txt</span></span></code></pre></div><p><strong>pip 安装缓慢</strong></p><p>更换国内镜像源：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span></span></code></pre></div><p><strong>验证 CUDA 支持</strong>：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>nvidia-smi  # 应显示 GPU 状态</span></span></code></pre></div><h3 id="步骤-3-安装vllm和模型依赖" tabindex="-1"><strong>步骤 3：安装vLLM和模型依赖</strong> <a class="header-anchor" href="#步骤-3-安装vllm和模型依赖" aria-label="Permalink to &quot;**步骤 3：安装vLLM和模型依赖**&quot;">​</a></h3><p><strong>安装vLLM（需CUDA 12.1）</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># Install vLLM from pip:</span></span>
<span class="line"><span>pip install vllm -i https://pypi.tuna.tsinghua.edu.cn/simple  # 使用清华镜像加速</span></span></code></pre></div><h4 id="方法3-安装flash-attention优化" tabindex="-1"><strong>方法3：安装Flash Attention优化</strong> <a class="header-anchor" href="#方法3-安装flash-attention优化" aria-label="Permalink to &quot;**方法3：安装Flash Attention优化**&quot;">​</a></h4><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># 在WSL虚拟环境中执行</span></span>
<span class="line"><span>pip uninstall -y vllm</span></span>
<span class="line"><span>pip install vllm[flash-attn]  # 安装含Flash Attention的版本</span></span></code></pre></div><p><strong>可选：安装Flash Attention优化</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip install flash-attn --no-build-isolation  # 提升推理速度（需GPU支持）</span></span></code></pre></div><h3 id="步骤-4-下载deepseek-r1-14b模型" tabindex="-1"><strong>步骤 4：下载DeepSeek-R1 14B模型</strong> <a class="header-anchor" href="#步骤-4-下载deepseek-r1-14b模型" aria-label="Permalink to &quot;**步骤 4：下载DeepSeek-R1 14B模型**&quot;">​</a></h3><p>vLLM支持多种主流的大模型格式，包括但不限于以下这些：</p><ul><li><strong>Aquila</strong></li><li><strong>Baichuan</strong></li><li><strong>BLOOM</strong></li><li><strong>Falcon</strong></li><li><strong>GPT-2</strong></li><li><strong>GPT BigCode</strong></li><li><strong>GPT-J</strong></li><li><strong>GPT-NeoX</strong></li><li><strong>InternLM</strong></li><li><strong>LLaMA</strong></li><li><strong>Mistral</strong></li><li><strong>MPT</strong></li><li><strong>OPT</strong></li><li><strong>Qwen</strong></li></ul><p>完整的支持模型列表可以查看vLLM的官方文档</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip install modelscope</span></span>
<span class="line"><span>export MODEL_DIR=/home/modern/models  # 替换为你的模型存储路径  # 确保此路径存在且可写</span></span>
<span class="line"><span>modelscope download --model deepseek-ai/DeepSeek-R1-Distill-Qwen-14B --local_dir $MODEL_DIR</span></span>
<span class="line"><span>modelscope download --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --cache_dir $MODEL_DIR</span></span>
<span class="line"><span>modelscope download --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --cache_dir /home/modern/models</span></span>
<span class="line"><span>modelscope download --model Qwen/Qwen2.5-Coder-7B-Instruct-AWQ --cache_dir /home/modern/models</span></span>
<span class="line"><span>#模型文件将被下载在&#39;cache_dir/Qwen/Qwen2-7b&#39;</span></span>
<span class="line"><span>模型文件将被下载在&#39;./local_dir&#39;</span></span></code></pre></div><p>下载单个文件到指定本地文件夹（以下载README.md到当前路径下“dir”目录为例）</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>modelscope download --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B README.md --local_dir ./dir</span></span></code></pre></div><p>无论是使用命令行还是ModelScope SDK，默认模型会下载到<code>~/.cache/modelscope/hub</code>目录下。如果需要修改cache目录，可以手动指定环境变量：MODELSCOPE_CACHE，指定后，模型将下载到该环境变量指定的目录中。</p><p>若下载慢，可使用Hugging Face镜像（修改<code>~/.bashrc</code>）：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>export HF_ENDPOINT=https://hf-mirror.com</span></span></code></pre></div><p>在你的用户主目录下创建一个<code>models</code>文件夹。在使用vLLM加载模型时，只需要指定模型所在目录的路径即可。例如，如果你的模型存放在<code>/home/your_username/models/llama-7b</code>目录下，那么在代码中可以这样指定模型路径</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>clear           #清屏</span></span></code></pre></div><h3 id="步骤-5-启动vllm推理服务" tabindex="-1"><strong>步骤 5：启动vLLM推理服务</strong> <a class="header-anchor" href="#步骤-5-启动vllm推理服务" aria-label="Permalink to &quot;**步骤 5：启动vLLM推理服务**&quot;">​</a></h3><p><strong>启动服务（根据显存调整参数）</strong>（关键参数说明）</p><p>--max-model-len 32768</p><ul><li>支持长达32k tokens的上下文（如长文档分析、代码生成）</li><li>显存消耗警告：32k序列的KV缓存需要约20GB显存（单卡），需双卡并行或使用A100 80GB。</li><li>必须处理<strong>超过2k的极长文本</strong>，且拥有双A100/A800。</li><li>显存不足（如单卡16GB）但需要勉强运行，此时需降低<code>--max-model-len</code>。</li></ul><p>--max-model-len=2048</p><ul><li>适合对话、摘要等常规任务（上下文≤2k）。</li><li><strong>显存优化</strong>：显存占用降低至8-10GB，轻松适配消费级显卡。</li></ul><p>依赖vLLM默认策略（通常0.9），但在极端长上下文下可能仍需手动调整。</p><p>--enforce-eager禁用CUDA Graph加速，可能降低推理速度（10-20%），仅建议在<strong>调试内存错误</strong>时启用。</p><p>--port=8102自定义端口便于多服务隔离，无性能影响。</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># Load and run the model:</span></span>
<span class="line"><span>//Qwen2.5-Coder-7B-Instruct-AWQ</span></span>
<span class="line"><span>vllm serve  /home/modern/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ  --tensor-parallel-size 1 --max-model-len 4096  --gpu-memory-utilization 0.9  --quantization awq --port=8102   --trust-remote-code  # 必须添加 </span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>  --trust-remote-code  # 必须添加 </span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span>(1) 启用批处理加速</span></span>
<span class="line"><span>  --enable-batch \\</span></span>
<span class="line"><span>  --max-num-batched-tokens 4096</span></span>
<span class="line"><span></span></span>
<span class="line"><span>  --tensor-parallel-size 1 \\                # GPU 并行数（根据 GPU 数量调整）</span></span>
<span class="line"><span>  --max-model-len 4096 \\                    # 最大上下文长度</span></span>
<span class="line"><span>  --gpu-memory-utilization 0.9              # GPU 显存利用率</span></span>
<span class="line"><span>  --quantization awq \\                      # 指定 AWQ 量化</span></span>
<span class="line"><span>  --trust-remote-code  # 必须添加</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>#tensor-parallel-size 2，这意味着它将模型在两张GPU上进行张量并行处理。</span></span>
<span class="line"><span># 假设模型路径为 /home/modern/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B</span></span>
<span class="line"><span># 提高显存利用率至合理范围（0.7-0.9）</span></span>
<span class="line"><span>CUDA_VISIBLE_DEVICES=0 vllm serve /home/modern/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B   </span></span>
<span class="line"><span>--port=8102   </span></span>
<span class="line"><span>--max-model-len=1024</span></span>
<span class="line"><span>--max-model-len=2048</span></span>
<span class="line"><span>#较大的max-model-len可以处理更长的上下文，但也会增加显存的占用。</span></span>
<span class="line"><span>#需要处理的上下文长度：如果必须处理长达32k tokens的文本，那么第一个命令的max-model-len是必须的，但需要确保显存足够,</span></span>
<span class="line"><span>#max-model-len是2048，这在大多数情况下可能已经足够，尤其是对于一般的对话或文本生成任务。适合对话、摘要等常规任务（上下文≤2k</span></span>
<span class="line"><span>--disable-log-stats</span></span>
<span class="line"><span>--gpu-memory-utilization=0.9 </span></span>
<span class="line"><span>#--gpu-memory-utilization=0.9，这意味着允许vLLM使用90%的GPU显存。</span></span>
<span class="line"><span>#gpu-memory-utilization，可以更精细地控制显存使用，避免OOM崩溃。</span></span>
<span class="line"><span>--trust-remote-code</span></span>
<span class="line"><span>      </span></span>
<span class="line"><span>--dtype=half    </span></span>
<span class="line"><span>--max-num-seqs=4</span></span>
<span class="line"><span>--max-num-seqs=8 \\            # 减少并发序列数（默认64）</span></span>
<span class="line"><span>--disable-log-stats \\          # 关闭统计日志以节省内存</span></span>
<span class="line"><span>--enforce-eager \\            </span></span>
<span class="line"><span>#--enforce-eager    # 禁用CUDA Graph优化，这会让PyTorch使用eager模式而不是更优化的cuda graph</span></span>
<span class="line"><span>--block-size=16               # 减小内存块分配粒度</span></span>
<span class="line"><span></span></span>
<span class="line"><span>--block-size 16 \\  # 提高KV缓存利用率（对长文本有效）</span></span>
<span class="line"><span>--swap-space 8 \\    # 启用CPU卸载，极端情况下扩展上下文 </span></span>
<span class="line"><span></span></span>
<span class="line"><span>#CUDA_VISIBLE_DEVICES=0指定只使用第0号GPU，也就是单卡运行</span></span>
<span class="line"><span>#命令指定了端口8102，而第一个没有指定，可能使用默认端口（通常是8000）。</span></span>
<span class="line"><span>#比如，7B模型，假设每个参数用2字节（比如半精度），参数量7B，那么模型本身占用的显存大约是7*10^9 * 2 bytes = 14 GB。</span></span>
<span class="line"><span></span></span>
<span class="line"><span>  </span></span>
<span class="line"><span>CUDA_VISIBLE_DEVICES=0 vllm serve \\</span></span>
<span class="line"><span>  --model $MODEL_DIR/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B \\</span></span>
<span class="line"><span>  --port 8102 \\</span></span>
<span class="line"><span>  --max-model-len 4096 \\</span></span>
<span class="line"><span>  --gpu-memory-utilization 0.9  # 显存利用率（0.9表示90%）</span></span></code></pre></div><p><strong>参数调整建议</strong></p><ul><li><strong>显存不足</strong>：添加 <code>--quantization awq</code>（需模型支持AWQ量化）</li><li><strong>长文本生成</strong>：增大 <code>--max-model-len</code>（但可能需更多显存）</li><li><strong>多GPU支持</strong>：设置 <code>CUDA_VISIBLE_DEVICES=0,1</code>（多卡并行）</li></ul><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>python3 -m vllm.entrypoints.openai.api_server --model /input0/Qwen-1_8B-Chat/ --host 0.0.0.0 --port 8080 --dtype auto --max-num-seqs 32 --max-model-len 4096 --tensor-parallel-size 1 --trust-remote-code</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># 转换模型为GGUF格式</span></span>
<span class="line"><span>python3 convert.py /home/modern/models/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B</span></span>
<span class="line"><span>./quantize /home/modern/models/deepseek-aigguf/gguf-model Q4_K_M</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 启动推理</span></span>
<span class="line"><span>./main -m //home/modern/models/deepseek-aigguf/gguf-model-q4_k_m.gguf -n 256 --gpu-layers 40</span></span></code></pre></div><h3 id="_1-环境清理与准备" tabindex="-1"><strong>1. 环境清理与准备</strong> <a class="header-anchor" href="#_1-环境清理与准备" aria-label="Permalink to &quot;**1. 环境清理与准备**&quot;">​</a></h3><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># 创建纯净虚拟环境（避免旧依赖干扰）</span></span>
<span class="line"><span>python -m venv vllm_env</span></span></code></pre></div><p><strong>1. 确认虚拟环境激活</strong></p><p>你正在使用名为 <code>vllm_env</code> 的虚拟环境。请先确保已激活该环境：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>source vllm_env/bin/activate  # Linux/Mac激活虚拟环境</span></span></code></pre></div><h3 id="‌2-安装指定版本的核心依赖" tabindex="-1">‌<strong>2. 安装指定版本的核心依赖</strong> <a class="header-anchor" href="#‌2-安装指定版本的核心依赖" aria-label="Permalink to &quot;‌**2. 安装指定版本的核心依赖**&quot;">​</a></h3><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># 安装基础兼容版本（关键冲突解决）</span></span>
<span class="line"><span>pip install numpy==1.26.4 </span></span>
<span class="line"><span>pip install  numba==0.58.1 </span></span>
<span class="line"><span>pip install  fsspec==2024.12.0 </span></span>
<span class="line"><span>pip install  transformers==4.47.1 </span></span>
<span class="line"><span>pip install  torch==2.1.2  # 确保CUDA兼容性‌:ml-citation{ref=&quot;1,6&quot; data=&quot;citationList&quot;}</span></span></code></pre></div><h3 id="‌3-安装vllm及关联库" tabindex="-1">‌<strong>3. 安装vLLM及关联库</strong> <a class="header-anchor" href="#‌3-安装vllm及关联库" aria-label="Permalink to &quot;‌**3. 安装vLLM及关联库**&quot;">​</a></h3><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># 安装vLLM 0.8.2（需与numpy&lt;2.0兼容）</span></span>
<span class="line"><span>pip install vllm==0.8.2</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 如果使用 AWQ 量化模型，需额外安装 autoawq（非必须，但 Qwen2.5-Coder-7B-Instruct-AWQ 需要）</span></span>
<span class="line"><span># 修复AutoAWQ冲突</span></span>
<span class="line"><span>pip install autoawq==0.2.8</span></span>
<span class="line"><span></span></span>
<span class="line"><span></span></span>
<span class="line"><span># 安装datasets 3.4.1（需匹配fsspec）</span></span>
<span class="line"><span>pip install datasets==3.4.1</span></span></code></pre></div><h3 id="_4-验证安装结果" tabindex="-1"><strong>4. 验证安装结果</strong> <a class="header-anchor" href="#_4-验证安装结果" aria-label="Permalink to &quot;**4. 验证安装结果**&quot;">​</a></h3><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># 检查关键库版本</span></span>
<span class="line"><span>pip show numpy numba transformers fsspec autoawq vllm</span></span>
<span class="line"><span>pip show datasets</span></span>
<span class="line"><span>pip show vllm</span></span></code></pre></div><p>预期输出</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>numpy==1.26.4</span></span>
<span class="line"><span>numba==0.58.1</span></span>
<span class="line"><span>transformers==4.47.1</span></span>
<span class="line"><span>fsspec==2024.12.0</span></span>
<span class="line"><span>autoawq==0.2.8</span></span>
<span class="line"><span>vllm==0.8.2</span></span></code></pre></div><h3 id="‌5-加速安装技巧" tabindex="-1">‌<strong>5. 加速安装技巧</strong> <a class="header-anchor" href="#‌5-加速安装技巧" aria-label="Permalink to &quot;‌**5. 加速安装技巧**&quot;">​</a></h3><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span># 使用阿里云镜像加速</span></span>
<span class="line"><span>pip install -i https://mirrors.aliyun.com/pypi/simple/ [包名]</span></span>
<span class="line"><span>#清华源</span></span>
<span class="line"><span>pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple</span></span></code></pre></div><h3 id="关键依赖关系表‌" tabindex="-1"><strong>关键依赖关系表</strong>‌ <a class="header-anchor" href="#关键依赖关系表‌" aria-label="Permalink to &quot;**关键依赖关系表**‌&quot;">​</a></h3><table tabindex="0"><thead><tr><th>库名称</th><th>要求版本</th><th>冲突原因</th><th>解决方案</th></tr></thead><tbody><tr><td>numpy</td><td>1.26.4</td><td>vllm/numba不兼容numpy≥2.0‌1</td><td>强制降级</td></tr><tr><td>numba</td><td>0.58.1</td><td>兼容numpy 1.26.x‌16</td><td>固定版本</td></tr><tr><td>transformers</td><td>4.47.1</td><td>autoawq版本限制‌12</td><td>降级至兼容范围</td></tr><tr><td>fsspec</td><td>2024.12.0</td><td>datasets版本限制‌13</td><td>安装旧版</td></tr></tbody></table><h3 id="_2-安装-vllm‌" tabindex="-1"><strong>2. 安装 vLLM</strong>‌ <a class="header-anchor" href="#_2-安装-vllm‌" aria-label="Permalink to &quot;**2. 安装 vLLM**‌&quot;">​</a></h3><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip list | grep vllm</span></span>
<span class="line"><span># 输出应包含类似：vllm 0.4.0</span></span>
<span class="line"><span>#vllm                              0.8.2</span></span></code></pre></div><h3 id="_3-检查-cuda-和-pytorch-兼容性‌" tabindex="-1"><strong>3. 检查 CUDA 和 PyTorch 兼容性</strong>‌ <a class="header-anchor" href="#_3-检查-cuda-和-pytorch-兼容性‌" aria-label="Permalink to &quot;**3. 检查 CUDA 和 PyTorch 兼容性**‌&quot;">​</a></h3><p>vLLM 需要 CUDA 和 PyTorch 支持。确保以下依赖已安装：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>#验证命令</span></span>
<span class="line"><span>nvcc --version</span></span>
<span class="line"><span># 检查 PyTorch 版本</span></span>
<span class="line"><span>python -c &quot;import torch; print(torch.__version__)&quot;</span></span>
<span class="line"><span># 检查 python 版本</span></span>
<span class="line"><span>python --version</span></span>
<span class="line"><span># 检查 PyTorch 是否支持 CUDA</span></span>
<span class="line"><span>python -c &quot;import torch; print(torch.cuda.is_available())&quot;</span></span>
<span class="line"><span># 输出应为：True</span></span>
<span class="line"><span></span></span>
<span class="line"><span># 检查 CUDA 版本（需要 &gt;= 11.8）</span></span>
<span class="line"><span>nvidia-smi | grep &quot;CUDA Version&quot;</span></span>
<span class="line"><span>#| NVIDIA-SMI 530.30.02              Driver Version: 531.14       CUDA Version: 12.1     |</span></span></code></pre></div><h3 id="步骤-6-测试api调用" tabindex="-1"><strong>步骤 6：测试API调用</strong> <a class="header-anchor" href="#步骤-6-测试api调用" aria-label="Permalink to &quot;**步骤 6：测试API调用**&quot;">​</a></h3><h4 id="通过python代码调用" tabindex="-1">通过Python代码调用 <a class="header-anchor" href="#通过python代码调用" aria-label="Permalink to &quot;通过Python代码调用&quot;">​</a></h4><p>检查本地目录是否包含以下关键文件：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>your_project_folder/</span></span>
<span class="line"><span>├── models/</span></span>
<span class="line"><span>│   └── Qwen/</span></span>
<span class="line"><span>│       └── Qwen2.5-Coder-7B-Instruct-AWQ/  # 重命名目录</span></span>
<span class="line"><span>│           ├── config.json</span></span>
<span class="line"><span>│           ├── tokenizer_config.json</span></span>
<span class="line"><span>│           ├── model.safetensors          # 合并后的单文件</span></span>
<span class="line"><span>│           └── tokenizer.json</span></span>
<span class="line"><span>├── scripts/                              # 新建脚本目录</span></span>
<span class="line"><span>│   └── qwen_inference.py</span></span>
<span class="line"><span>└── vllm_env/                            # 仅存放虚拟环境</span></span></code></pre></div><p>然后，您可以利用 <a href="https://platform.openai.com/docs/api-reference/chat/completions/create" target="_blank" rel="noreferrer">create chat interface</a> 来与Qwen进行对话：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>from openai import OpenAI</span></span>
<span class="line"><span># Set OpenAI&#39;s API key and API base to use vLLM&#39;s API server.</span></span>
<span class="line"><span>openai_api_key = &quot;EMPTY&quot;</span></span>
<span class="line"><span>openai_api_base = &quot;http://localhost:8000/v1&quot;</span></span>
<span class="line"><span></span></span>
<span class="line"><span>client = OpenAI(</span></span>
<span class="line"><span>    api_key=openai_api_key,</span></span>
<span class="line"><span>    base_url=openai_api_base,</span></span>
<span class="line"><span>)</span></span>
<span class="line"><span></span></span>
<span class="line"><span>chat_response = client.chat.completions.create(</span></span>
<span class="line"><span>    model=&quot;Qwen/Qwen2.5-7B-Instruct&quot;,</span></span>
<span class="line"><span>    messages=[</span></span>
<span class="line"><span>        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are Qwen, created by Alibaba Cloud. You are a helpful assistant.&quot;},</span></span>
<span class="line"><span>        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me something about large language models.&quot;},</span></span>
<span class="line"><span>    ],</span></span>
<span class="line"><span>    temperature=0.7,</span></span>
<span class="line"><span>    top_p=0.8,</span></span>
<span class="line"><span>    max_tokens=512,</span></span>
<span class="line"><span>    extra_body={</span></span>
<span class="line"><span>        &quot;repetition_penalty&quot;: 1.05,</span></span>
<span class="line"><span>    },</span></span>
<span class="line"><span>)</span></span>
<span class="line"><span>print(&quot;Chat response:&quot;, chat_response)</span></span></code></pre></div><h2 id="上下文支持扩展" tabindex="-1">上下文支持扩展 <a class="header-anchor" href="#上下文支持扩展" aria-label="Permalink to &quot;上下文支持扩展&quot;">​</a></h2><p>Qwen2.5 模型的上下文长度默认设置为 3 2768 个token。为了处理超出 3 2768 个token的大量输入，我们使用了 <a href="https://arxiv.org/abs/2309.00071" target="_blank" rel="noreferrer">YaRN</a>，这是一种增强模型长度外推的技术，确保在处理长文本时的最优性能。</p><p>vLLM 支持 YaRN，并且可以通过在模型的 <code>config.json</code> 文件中添加一个 <code>rope_scaling</code> 字段来启用它。例如，</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>{</span></span>
<span class="line"><span>  ...,</span></span>
<span class="line"><span>  &quot;rope_scaling&quot;: {</span></span>
<span class="line"><span>    &quot;factor&quot;: 4.0,</span></span>
<span class="line"><span>    &quot;original_max_position_embeddings&quot;: 32768,</span></span>
<span class="line"><span>    &quot;type&quot;: &quot;yarn&quot;</span></span>
<span class="line"><span>  }</span></span>
<span class="line"><span>}</span></span></code></pre></div><p><strong>激活虚拟环境</strong>：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>source vllm_env/bin/activate</span></span></code></pre></div><p><strong>运行脚本</strong>：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>cd scripts</span></span>
<span class="line"><span>python qwen_inference.py</span></span>
<span class="line"><span>python chat.py</span></span></code></pre></div><p>问题1:</p><p>prompts = [&quot;Write a Python function to calculate factorial:&quot;] outputs = llm.generate(prompts, sampling_params)</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>\`\`\`python</span></span>
<span class="line"><span>def factorial(n):</span></span>
<span class="line"><span>    # Base case: factorial of 0 is 1</span></span>
<span class="line"><span>    if n == 0:</span></span>
<span class="line"><span>        return 1</span></span>
<span class="line"><span>    # Initialize result to 1</span></span>
<span class="line"><span>    result = 1</span></span>
<span class="line"><span>    # Loop through numbers from 1 to n</span></span>
<span class="line"><span>    for i in range(1, n + 1):</span></span>
<span class="line"><span>        result *= i  # Multiply result by current number i</span></span>
<span class="line"><span>    return result</span></span>
<span class="line"><span></span></span>
<span class="line"><span># Test the function</span></span>
<span class="line"><span>print(factorial(5))  # Output: 120</span></span>
<span class="line"><span>print(factorial(0))  # Output: 1</span></span>
<span class="line"><span>print(factorial(3))  # Output: 6</span></span>
<span class="line"><span>\`\`\`</span></span></code></pre></div><p>在WSL或Windows中创建 <code>test_api.py</code>：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>prompt=&quot;如何学习人工智能？&quot;,</span></span></code></pre></div><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 初始化客户端（注意端口与启动服务时一致）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    base_url</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;http://localhost:8102/v1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 本地服务地址</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    api_key</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;EMPTY&quot;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # vLLM无需认证</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 生成文本</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">response </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.completions.create(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-14B&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    prompt</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;如何高效学习深度学习？&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    max_tokens</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">200</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,    </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 生成的最大token数</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    temperature</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.7</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,   </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 控制随机性（0-1，越大越随机）</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    top_p</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.9</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,         </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 核采样阈值（通常与temperature配合）</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    stop</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]        </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 停止生成的条件（如遇到换行符则停止）</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(response.choices[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].text)</span></span></code></pre></div><p><strong>运行测试</strong></p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>pip install openai  # 确保已安装</span></span>
<span class="line"><span>python test_api.py</span></span></code></pre></div><h4 id="_2-2-通过curl命令测试" tabindex="-1">2.2 通过curl命令测试 <a class="header-anchor" href="#_2-2-通过curl命令测试" aria-label="Permalink to &quot;2.2 通过curl命令测试&quot;">​</a></h4><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">curl</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> http://localhost:8000/v1/completions</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  -H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Content-Type: application/json&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">   -d</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;{  &quot;model&quot;:&quot;/home/modern/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&quot;,    &quot;prompt&quot;: &quot;中国的首都是哪里？&quot;,    &quot;max_tokens&quot;: 50  }&#39;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  </span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Call the server using curl:</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">curl</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -X</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> POST</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;http://localhost:8000/v1/chat/completions&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">	-H</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &quot;Content-Type: application/json&quot;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">	--data</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">		&quot;model&quot;:&quot;/home/modern/models/Qwen/Qwen2.5-Coder-7B-Instruct-AWQ&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">		&quot;messages&quot;: [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">			{</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">				&quot;role&quot;: &quot;user&quot;,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">				&quot;content&quot;: &quot;What is the capital of France?&quot;</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">			}</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">		],</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">		&quot;stream&quot;: true,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">		&quot;temperature&quot;: 0.1</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">		</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">	}&#39;</span></span></code></pre></div><h3 id="步骤-3-验证服务状态" tabindex="-1"><strong>步骤 3：验证服务状态</strong> <a class="header-anchor" href="#步骤-3-验证服务状态" aria-label="Permalink to &quot;**步骤 3：验证服务状态**&quot;">​</a></h3><h4 id="_3-1-检查服务日志" tabindex="-1">3.1 检查服务日志 <a class="header-anchor" href="#_3-1-检查服务日志" aria-label="Permalink to &quot;3.1 检查服务日志&quot;">​</a></h4><ul><li>服务启动后，终端会持续输出日志，观察是否有以下关键信息：</li></ul><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Uvicorn running on http://0.0.0.0:8102 (Press CTRL+C to quit)</span></span>
<span class="line"><span>NVIDIA CUDA initialized successfully  # 确认CUDA可用</span></span>
<span class="line"><span>Model loaded in 23.5s                  # 模型加载完成</span></span></code></pre></div><h4 id="_3-2-查看显存占用" tabindex="-1">3.2 查看显存占用 <a class="header-anchor" href="#_3-2-查看显存占用" aria-label="Permalink to &quot;3.2 查看显存占用&quot;">​</a></h4><p>在另一个WSL终端中运行：</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">watch</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -n</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> nvidia-smi</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # 每秒刷新显存占用情况</span></span></code></pre></div><p>正常情况：显存占用应接近 <code>--gpu-memory-utilization</code> 设置的比例（如90%）。</p><h3 id="步骤-4-高级用法" tabindex="-1"><strong>步骤 4：高级用法</strong> <a class="header-anchor" href="#步骤-4-高级用法" aria-label="Permalink to &quot;**步骤 4：高级用法**&quot;">​</a></h3><h4 id="_4-1-流式输出-streaming" tabindex="-1">4.1 流式输出（Streaming） <a class="header-anchor" href="#_4-1-流式输出-streaming" aria-label="Permalink to &quot;4.1 流式输出（Streaming）&quot;">​</a></h4><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>response = client.completions.create(</span></span>
<span class="line"><span>    model=&quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-14B&quot;,</span></span>
<span class="line"><span>    prompt=&quot;请写一首关于春天的诗：&quot;,</span></span>
<span class="line"><span>    stream=True  # 启用流式输出</span></span>
<span class="line"><span>)</span></span>
<span class="line"><span></span></span>
<span class="line"><span>for chunk in response:</span></span>
<span class="line"><span>    print(chunk.choices[0].text, end=&quot;&quot;, flush=True)</span></span></code></pre></div><h4 id="_4-2-批量推理" tabindex="-1">4.2 批量推理 <a class="header-anchor" href="#_4-2-批量推理" aria-label="Permalink to &quot;4.2 批量推理&quot;">​</a></h4><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>batch_prompts = [</span></span>
<span class="line"><span>    &quot;深度学习的三大基础概念是：&quot;,</span></span>
<span class="line"><span>    &quot;如何预防感冒？&quot;,</span></span>
<span class="line"><span>    &quot;Python的GIL是什么？&quot;</span></span>
<span class="line"><span>]</span></span>
<span class="line"><span></span></span>
<span class="line"><span>for prompt in batch_prompts:</span></span>
<span class="line"><span>    response = client.completions.create(</span></span>
<span class="line"><span>        model=&quot;deepseek-ai/DeepSeek-R1-Distill-Qwen-14B&quot;,</span></span>
<span class="line"><span>        prompt=prompt,</span></span>
<span class="line"><span>        max_tokens=100</span></span>
<span class="line"><span>    )</span></span>
<span class="line"><span>    print(f&quot;问题：{prompt}\\n回答：{response.choices[0].text}\\n&quot;)</span></span></code></pre></div><h3 id="常见问题解决" tabindex="-1"><strong>常见问题解决</strong> <a class="header-anchor" href="#常见问题解决" aria-label="Permalink to &quot;**常见问题解决**&quot;">​</a></h3><ol><li><p><strong>CUDA不可用</strong></p><ul><li>检查NVIDIA驱动版本：<code>nvidia-smi</code>（在WSL中应能识别GPU）</li><li>确保CUDA路径正确：<code>echo $PATH | grep cuda</code></li></ul></li><li><p><strong>模型加载失败</strong></p><ul><li>检查模型路径权限：<code>chmod -R 755 $MODEL_DIR</code></li><li>确认模型文件完整（下载中断时重新运行<code>modelscope download</code>）</li><li>检查模型路径是否正确（区分大小写）</li><li>确保模型文件完整（对比Hugging Face仓库的文件列表）</li></ul></li><li><p><strong>显存不足</strong></p><ul><li>降低<code>--max-model-len</code>（如2048）</li><li>关闭其他占用GPU的程序（如Windows游戏、录屏软件）</li></ul><p><strong>API调用超时</strong></p><ul><li>增加 <code>--max-num-seqs 32</code>（服务启动参数，提高并发处理能力）</li><li>减少 <code>max_tokens</code> 值</li></ul><p><strong>生成内容质量差</strong></p><ul><li>调整 <code>temperature</code>（降低值使输出更确定性）</li><li>设置 <code>top_k=50</code> 或 <code>top_p=0.9</code> 限制采样范围</li></ul></li></ol><h3 id="性能优化建议" tabindex="-1"><strong>性能优化建议</strong> <a class="header-anchor" href="#性能优化建议" aria-label="Permalink to &quot;**性能优化建议**&quot;">​</a></h3><ol><li><p><strong>启用连续批处理</strong> 添加 <code>--enforce-eager</code> 到启动参数（减少显存碎片）</p></li><li><p><strong>持久化服务</strong> 使用 <code>tmux</code> 或 <code>systemd</code> 保持服务后台运行：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>tmux new -s vllm</span></span>
<span class="line"><span># 启动服务后按 Ctrl+B, 再按 D 退出会话</span></span>
<span class="line"><span># 重新连接：tmux attach -t vllm</span></span></code></pre></div></li><li><p><strong>模型量化</strong> 如果使用AWQ量化版模型（需重新下载）：</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>vllm serve --model path/to/model --quantization awq</span></span></code></pre></div></li></ol><h3 id="优化建议" tabindex="-1"><strong>优化建议</strong> <a class="header-anchor" href="#优化建议" aria-label="Permalink to &quot;**优化建议**&quot;">​</a></h3><ul><li>将模型存储在WSL的ext4分区（默认路径如<code>/home/</code>），避免挂载Windows NTFS目录（I/O性能差）。</li><li>使用<code>tmux</code>或<code>screen</code>保持服务后台运行：</li></ul><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>sudo apt install tmux</span></span>
<span class="line"><span>tmux new -s vllm</span></span>
<span class="line"><span># 启动服务后按Ctrl+B, 再按D退出会话，需恢复时运行 tmux attach -t vllm</span></span></code></pre></div><p>完成上述步骤后，你的 DeepSeek-R1 14B 模型即可通过 API 提供服务。建议先从简单问题测试，逐步调整参数以适应实际需求。</p>`,178)]))}const k=a(i,[["render",t]]);export{u as __pageData,k as default};
