import{_ as r,c as o,o as t,ag as a}from"./chunks/framework.oP1PDRBo.js";const h=JSON.parse('{"title":"DeepSeek+ Ollama+ Cherry Studio搭建本地私有知识库","description":"","frontmatter":{"Generator":"Microsoft Word 15","lang":"ZH-CN","Originator":"Microsoft Word 15","ProgId":"Word.Document"},"headers":[],"relativePath":"ollama/Cherry Studio知识库.md","filePath":"ollama/Cherry Studio知识库.md","lastUpdated":1744012294000}'),p={name:"ollama/Cherry Studio知识库.md"};function l(s,e,n,i,g,d){return t(),o("div",null,e[0]||(e[0]=[a('<h1 id="deepseek-ollama-cherry-studio搭建本地私有知识库" tabindex="-1">DeepSeek+ Ollama+ Cherry Studio搭建本地私有知识库 <a class="header-anchor" href="#deepseek-ollama-cherry-studio搭建本地私有知识库" aria-label="Permalink to &quot;DeepSeek+ Ollama+ Cherry Studio搭建本地私有知识库&quot;">​</a></h1><h2 id="一、什么是知识库" tabindex="-1">一、什么是知识库？ <a class="header-anchor" href="#一、什么是知识库" aria-label="Permalink to &quot;一、什么是知识库？&quot;">​</a></h2><p>知识库（Knowledge Base）是一个存储和管理知识的系统，通常包含结构化和非结构化的信息，用于帮助用户或系统快速查找和获取相关知识。</p><p>你可以把它想象成一个&quot;知识仓库&quot;，里面存放着各种有用的信息，比如文档、常见问题解答（FAQ）、数据库、规则、案例等。</p><p><strong>举个栗子：</strong></p><p>假设你是一名技术客服人员，客户问你一个关于某个产品的问题。 如果你有一个完善的知识库，你可以直接在知识库中搜索相关产品的使用说明或常见问题解答，快速找到答案并回复客户。 如果没有知识库，你可能需要去问同事或查找一堆文档，效率会低很多。</p><h2 id="二、为什么要搭建个人知识库" tabindex="-1">二、为什么要搭建个人知识库？ <a class="header-anchor" href="#二、为什么要搭建个人知识库" aria-label="Permalink to &quot;二、为什么要搭建个人知识库？&quot;">​</a></h2><p>搭建个人知识库不仅能提升工作效率，还能促进持续学习和团队协作，对个人职业发展和团队知识管理具有长远意义。</p><ol><li><strong>知识系统化</strong></li></ol><ul><li><strong>集中管理</strong>：将零散的知识点整合到一个平台，便于查找和使用。</li><li><strong>结构化存储</strong>：通过分类和标签，使知识更易管理和检索。</li></ul><ol start="2"><li><strong>提升效率</strong></li></ol><ul><li><strong>快速检索</strong>：通过搜索功能迅速找到所需信息，减少重复查找时间。</li><li><strong>减少重复劳动</strong>：避免重复解决相同问题，提升工作效率。</li></ul><ol start="3"><li><strong>持续学习</strong></li></ol><ul><li><strong>积累经验</strong>：记录问题和解决方案，形成个人经验库。</li><li><strong>知识更新</strong>：持续更新知识库，保持技术前沿。</li></ul><ol start="4"><li><strong>团队协作</strong></li></ol><ul><li><strong>知识共享</strong>：团队成员可访问知识库，促进经验交流。</li><li><strong>减少沟通成本</strong>：常见问题和解决方案在知识库中共享，减少重复沟通。</li></ul><p>如果想要搭建知识库，这里我们又不得不提到另外一个词------RAG.</p><h2 id="三、什么是rag" tabindex="-1">三、什么是RAG？ <a class="header-anchor" href="#三、什么是rag" aria-label="Permalink to &quot;三、什么是RAG？&quot;">​</a></h2><p><strong>RAG</strong>（Retrieval Augmented Generation，检索增强生成）是一种让大语言模型（LLM）变得更聪明的方法。</p><p>简单来说，它通过给模型提供一个外部的&quot;知识库&quot;，让模型在回答问题时可以&quot;查资料&quot;，从而给出更准确、更相关的答案。</p><p><strong>举个栗子：</strong></p><p>想象一下，大模型就像一个学生，而RAG系统就是一本字典。 当学生遇到不懂的问题时，他可以翻开字典查找相关的解释，然后再根据字典里的内容回答问题。 这样，学生不仅能回答得更准确，还能避免&quot;瞎编&quot;答案。 那么，当大模型遇到不懂的问题时，他也可以查字典，也就是RAG系统。</p><h2 id="四、有了大模型-为什么还需要rag" tabindex="-1">四、有了大模型，为什么还需要RAG？ <a class="header-anchor" href="#四、有了大模型-为什么还需要rag" aria-label="Permalink to &quot;四、有了大模型，为什么还需要RAG？&quot;">​</a></h2><ol><li>大模型的知识有限：大模型的知识主要来自它训练时用的数据，而这些数据是有限的，尤其是企业内部的业务知识或产品信息，模型可能完全不了解。</li><li>微调成本高：如果想让大模型学习企业特定的知识，通常需要微调模型，但这不仅成本高，而且效果也不一定好。</li><li>幻觉问题：大模型在不熟悉的领域可能会&quot;瞎编&quot;答案，这在企业应用中是不可接受的，尤其是那些需要准确信息的场景。</li></ol><p>这里，我先贴出一个 RAG 的运作流程图。</p><p>好了，废话不多说，接下来就带大家具体实操了，</p><p>如何用DeepSeek+ Ollama+ Cherry Studio在本地搭建私有知识库。</p><h2 id="五、搭建实操" tabindex="-1">五、搭建实操 <a class="header-anchor" href="#五、搭建实操" aria-label="Permalink to &quot;五、搭建实操&quot;">​</a></h2><p><strong>1. 软硬件配置说明</strong></p><p>（1）硬件: 公司标配的办公电脑 Apple M3 Pro 36GB</p><p>（2）软件：cherry studio v0.9.21 ollama</p><p>（3）基础模型为：deepseek-r1:7b （ollama内置下载）</p><p>（4）嵌入模型为：bge-m3 （ollama内置下载）</p><p><strong>2. 步骤说明</strong></p><p><strong>步骤[一]：下载安装Cherry Studio可视化工具</strong></p><p>官网直接下载安装，毋庸赘述。 cherry studio v0.9.21</p><p><strong>步骤二：部署DeepSeek模型 + Embedding模型</strong></p><p>那么，这里又分为两种方式</p><p>**方式[一]：使用 Ollama 运行 DeepSeek R1 **</p><p><strong>好处：</strong> 可以实现本地部署DeepSeek模型 +Embedding模型，免费、安全</p><p><strong>坏处：</strong> 受到本地电脑配置的限制，只能使用蒸馏版，推理能力远不如满血版，本文以 DeepSeek R1 7B 版本为例。</p><p><strong>方式二：调用第三方平台服务 API</strong></p><p><strong>好处：</strong> 不用本地部署，可以使用完整的模型服务</p><p><strong>坏处：</strong> 调用云端的模型服务需要一定的费用，但是也不贵。比如硅基流动，不过实测最近硅基api很卡，有时比ds 官网还卡。</p><p>因为我自己搭建的知识库会用到很多内部资料，出于安全性考虑今天我们重点介绍的是第一种方式<strong>使用Ollama运行DeepSeek R1 ，借助Cherry Studio嵌入本地资料，搭建本地私有知识库。</strong></p><p>**本地安装ollama并下载需要的模型。 并安装后，打开任意终端， 如图输入以下两个命令来下载并运行 deepseek-r1 和beg-m3，基础模型为deepseek-r1:7b， 嵌入模型为bge-m3， 需要联网下载，时间会比较久， 耐心等待提示成功即可。</p><p><strong>ollama</strong> run deepseek-r1:7b**</p><p>运行 deepseek-r1 成功后，可以再终端进行问答测试</p><p><strong>ollama</strong> pull bge-m3**</p><p><strong>步骤三：在 Cherry Studio 中配置模型</strong></p><p>如图，在 Cherry Studio 设置页面配置模型服务，选择 Ollama， 模型选择弹窗中选中 deepseek-r1:7b 和 bge-m3:latest</p><p>配置模型成功后应该如下图所示，也可以点击 check 按钮进行验证。</p><p><strong>步骤四：在 Cherry Studio 中构建知识库</strong></p><p>在Cherry Studio知识库配置页卡中，按照自己的需要新建【知识库】，嵌入模型选择【bge-m3:latest】， 然后上传知识库素材，可以添加文档(支持pdf/doc/xlsx/ppt/txt/md等)，也可以**添加目录（这个极其方便）、网站等，**添加完以后出现绿色的对号，表示向量化完成。</p><p>添加素材完成后，可以使用【搜索知识库】进行测试验证，如下图所示，我这里提供的素材是 clb串流原理的文档，所以提问了【clb为什么会串流？】。模型会根据知识库中的素材进行回答，并标注来源和匹配度等。</p><p><strong>步骤五：在 Cherry Studio 中构建智能助手</strong></p><p>经过上面的步骤，我们的大模型和个人知识库其实已经搭建好了，后续有新的素材可以重复步骤四补充进去，让知识库越来越丰富。接下来就是通过Cherry Studio 提供的聊天助手来更加方便的使用我们的知识库了。</p><p>如图可以使用默认的聊天助手，也可以新建一个自定义的聊天助手，选择本地模型和知识库， 这样默认这个聊天助手所有的问答都会参考我们的知识库资料。同理也可以搭建多个知识库和多个聊天助手，满足不同的使用场景。 其实 Cherry Studio中已经默认配置了很多助手，比如产品助手、运维助手、软件开发助手等等，大家可以按需使用。</p><p>当然也可以在聊天页面，选择使用哪个知识库进行推理，或者不使用知识库，让大模型基于通用知识进行推理。</p><p>好了，到这里相信你已经拥有了属于自己的知识库和智能助手，预祝你可以在 AI时代，更加智能和高效的工作。</p><p>另外，受限于工作机配置原因，我们这里使用的是 ds-r1:7b版本，如果想要使用满血版的 deepseek，可以参照步骤二中的方式[一]使用付费服务或者自己购买腾讯云HAI进行部署。如果需要使用其他大模型，部署步骤和 deepseek类似，可以根据自己的电脑配置，酌情选择。</p><h2 id="六、未来已来-重新定义程序员的战场" tabindex="-1">六、未来已来：重新定义程序员的战场 <a class="header-anchor" href="#六、未来已来-重新定义程序员的战场" aria-label="Permalink to &quot;六、未来已来：重新定义程序员的战场&quot;">​</a></h2><p>历史上，C语言没有淘汰汇编工程师，而是让他们转向嵌入式开发；云计算没有让运维消失，而是催生了DevOps专家。</p><p><strong>AI时代程序员的生存法则</strong>：</p><ul><li>将DeepSeek视为&quot;外接大脑&quot;，专注<strong>需求抽象</strong>（从业务到数学模型）</li><li>修炼<strong>系统思维</strong>（复杂问题拆解能力）</li><li>保持<strong>技术敏锐度</strong>（通过联网搜索持续进化）</li></ul><p>正如DeepSeek-R1在解答数学难题时需要172秒的深度思考，程序员真正的价值，在于<strong>在混沌中定义问题边界的能力</strong>------而这正是AI至今未能突破的&quot;人类智慧结界&quot;。</p><p><strong>记住：淘汰你的从来不是AI，而是会用AI的其他程序员。</strong></p><p></p>',68)]))}const m=r(p,[["render",l]]);export{h as __pageData,m as default};
